-- build the docker image, start the container and 1-2 more consoles

docker build -t brost .
-- forwarding the kafka ports so i can connect from my laptop,too
docker run -p 2181:2181 -p 9092:9092 -p 9093:9093 -p 33000:3000 -ti brost bash

docker ps -- note the container id
docker exec -i -t caf5c0a9167a bash

-- there is already a shell script running that inserts random orders
less db-insert.sh

-- let's see what's happening in mysql
mysql code

mysql> describe orders;
mysql> select * from oders limit 42;
mysql> insert into orders (product, price, user_id) values ('lumpy', 100, 42);

-- quick view at the maxwell config (it's already running)
cat /usr/local/bin/start-maxwell.sh

-- we can product to stdout to see the json pieces
/maxwell-1.12.0/bin/maxwell --user='maxwell' --password='maxwell' --host='127.0.0.1' --producer=stdout

-- initialize the datagen topics
./init-datagen.sh

-- show that all our topics are there
kafka-topics --zookeeper localhost:2181 --list

-- start ksql-cli and initialize the clickstream topics
ksql-cli local
run script '/usr/share/doc/ksql-clickstream-demo/clickstream-schema.sql';

list topics;
list streams;
list tables;


-- now set up out orders table step by step

create stream orders_raw (data map(varchar, varchar)) with (kafka_topic = 'maxwell_code_orders', value_format = 'JSON');

create stream orders_flat as select data['id'] as id, data['product'] as product, data['price'] as price, data['user_id'] as user_id from orders_raw;

create stream orders as select cast(id as integer) as id, product, cast(price as bigint) as price, cast(user_id as integer) as user_id from orders_flat;

-- i would rather do this if it wouldn't fail
--create stream orders_fails as select cast(data['id'] as integer) as id from orders_raw ;

-- also, this is broken
--create stream orders_partby as select cast(id as integer) as id, product, cast(price as integer) as price, cast(user_id as integer) as user_id from orders_flat partition by id;


select product, count(*), sum(price) from orders window tumbling (size 15 seconds) group by product;

-- now the table
--create table orders_per_15s as select product, count(*), sum(price) from orders window tumbling (size 15 seconds) group by product;

--create table orders_per_min as select product, sum(price) amount from orders window tumbling (size 60 seconds) group by product;
create table orders_per_min as select product, sum(price) amount from orders window hopping (size 60 seconds, advance by 15 seconds) group by product;


CREATE TABLE orders_per_min_ts as select rowTime as event_ts, * from orders_per_min;

--create table spending_per_min as select user_id, sum(price) amount from orders window tumbling (size 2 minutes) group by user_id ;

--create table user_tally as select user_id, sum(price) amount from orders group by user_id;


-- now load the streams into elastic and grafana
 cd /usr/share/doc/ksql-clickstream-demo/




 http://localhost:33000/dashboard/db/click-stream-analysis




-- random notes. here be dragons
CREATE STREAM USER_CLICKSTREAM_ORDER AS SELECT userid, u.username, ip, u.city, request, status, bytes, o.product, o.price FROM clickstream c LEFT JOIN web_users u ON c.userid = u.user_id LEFT JOIN orders o on c.userid = o.user_id



No SUM aggregate function with Schema{INT32}  argument type exists!
